{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-10T03:45:36.566251Z","iopub.status.busy":"2024-10-10T03:45:36.565825Z","iopub.status.idle":"2024-10-10T03:45:43.528265Z","shell.execute_reply":"2024-10-10T03:45:43.527271Z","shell.execute_reply.started":"2024-10-10T03:45:36.566152Z"},"trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","import keras\n","from keras.preprocessing import image\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import cv2\n","from tensorflow.keras.applications import VGG16, InceptionResNetV2\n","from keras import regularizers\n","from tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-10T03:45:43.530046Z","iopub.status.busy":"2024-10-10T03:45:43.529735Z","iopub.status.idle":"2024-10-10T03:45:43.534419Z","shell.execute_reply":"2024-10-10T03:45:43.533140Z","shell.execute_reply.started":"2024-10-10T03:45:43.530017Z"},"trusted":true},"outputs":[],"source":["train_dir = \"train/\" \n","test_dir = \"test/\"   "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-10T03:45:43.536620Z","iopub.status.busy":"2024-10-10T03:45:43.536266Z","iopub.status.idle":"2024-10-10T03:45:43.954645Z","shell.execute_reply":"2024-10-10T03:45:43.953532Z","shell.execute_reply.started":"2024-10-10T03:45:43.536584Z"},"trusted":true},"outputs":[],"source":["model= tf.keras.models.Sequential()\n","model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(168, 168,1)))\n","model.add(Conv2D(64,(3,3), padding='same', activation='relu' ))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(128,(5,5), padding='same', activation='relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","    \n","model.add(Conv2D(512,(3,3), padding='same', activation='relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","model.add(Conv2D(512,(3,3), padding='same', activation='relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten()) \n","model.add(Dense(256,activation = 'relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","    \n","model.add(Dense(512,activation = 'relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","\n","model.add(Dense(7, activation='softmax'))"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-10T03:45:43.956891Z","iopub.status.busy":"2024-10-10T03:45:43.956460Z","iopub.status.idle":"2024-10-10T03:45:43.961020Z","shell.execute_reply":"2024-10-10T03:45:43.960013Z","shell.execute_reply.started":"2024-10-10T03:45:43.956855Z"},"trusted":true},"outputs":[],"source":["img_size=168"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-10T03:45:43.963163Z","iopub.status.busy":"2024-10-10T03:45:43.962728Z","iopub.status.idle":"2024-10-10T03:45:43.973725Z","shell.execute_reply":"2024-10-10T03:45:43.972701Z","shell.execute_reply.started":"2024-10-10T03:45:43.963118Z"},"trusted":true},"outputs":[],"source":["train_datagen = ImageDataGenerator(width_shift_range = 0.1,\n","                                         height_shift_range = 0.1,\n","                                         horizontal_flip = True,\n","                                         rescale = 1./255,\n","                                         validation_split = 0.2\n","                                        )\n","validation_datagen = ImageDataGenerator(rescale = 1./255,\n","                                         validation_split = 0.2)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-10T03:45:43.975677Z","iopub.status.busy":"2024-10-10T03:45:43.975249Z","iopub.status.idle":"2024-10-10T03:46:17.134985Z","shell.execute_reply":"2024-10-10T03:46:17.133869Z","shell.execute_reply.started":"2024-10-10T03:45:43.975633Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 22968 images belonging to 7 classes.\n","Found 1432 images belonging to 7 classes.\n"]}],"source":["train_generator = train_datagen.flow_from_directory(directory = train_dir,\n","                                                    target_size = (img_size,img_size),\n","                                                    batch_size = 64,\n","                                                    color_mode = \"grayscale\",\n","                                                    class_mode = \"categorical\",\n","                                                    subset = \"training\"\n","                                                   )\n","validation_generator = validation_datagen.flow_from_directory( directory = test_dir,\n","                                                              target_size = (img_size,img_size),\n","                                                              batch_size = 64,\n","                                                              color_mode = \"grayscale\",\n","                                                              class_mode = \"categorical\",\n","                                                              subset = \"validation\"\n","                                                             )"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-10T03:46:17.136643Z","iopub.status.busy":"2024-10-10T03:46:17.136359Z","iopub.status.idle":"2024-10-10T03:46:17.153412Z","shell.execute_reply":"2024-10-10T03:46:17.152432Z","shell.execute_reply.started":"2024-10-10T03:46:17.136614Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Tareq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]}],"source":["model.compile(\n","    optimizer = Adam(lr=0.001), \n","    loss='categorical_crossentropy', \n","    metrics=['accuracy']\n","  )"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-10T03:46:17.156602Z","iopub.status.busy":"2024-10-10T03:46:17.156042Z","iopub.status.idle":"2024-10-10T03:46:17.160627Z","shell.execute_reply":"2024-10-10T03:46:17.159625Z","shell.execute_reply.started":"2024-10-10T03:46:17.156547Z"},"trusted":true},"outputs":[],"source":["epochs = 2\n","batch_size = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-10T03:46:17.162680Z","iopub.status.busy":"2024-10-10T03:46:17.162395Z","iopub.status.idle":"2024-10-10T03:56:21.328833Z","shell.execute_reply":"2024-10-10T03:56:21.327123Z","shell.execute_reply.started":"2024-10-10T03:46:17.162653Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","  3/359 [..............................] - ETA: 4:53:22 - loss: 3.2544 - accuracy: 0.1458"]}],"source":["history = model.fit(x = train_generator,epochs = epochs,validation_data = validation_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-10T03:56:21.330117Z","iopub.status.idle":"2024-10-10T03:56:21.330576Z"},"trusted":true},"outputs":[],"source":["fig , ax = plt.subplots(1,2)\n","train_acc = history.history['accuracy']\n","train_loss = history.history['loss']\n","fig.set_size_inches(12,4)\n","\n","ax[0].plot(history.history['accuracy'])\n","ax[0].plot(history.history['val_accuracy'])\n","ax[0].set_title('Training Accuracy vs Validation Accuracy')\n","ax[0].set_ylabel('Accuracy')\n","ax[0].set_xlabel('Epoch')\n","ax[0].legend(['Train', 'Validation'], loc='upper left')\n","\n","ax[1].plot(history.history['loss'])\n","ax[1].plot(history.history['val_loss'])\n","ax[1].set_title('Training Loss vs Validation Loss')\n","ax[1].set_ylabel('Loss')\n","ax[1].set_xlabel('Epoch')\n","ax[1].legend(['Train', 'Validation'], loc='upper left')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-10T03:56:21.332042Z","iopub.status.idle":"2024-10-10T03:56:21.332709Z"},"trusted":true},"outputs":[],"source":["model.save('model.keras')"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.status.busy":"2024-10-10T03:56:21.334103Z","iopub.status.idle":"2024-10-10T03:56:21.334745Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","\n","img_height = 168\n","img_width = 168\n","\n","# Preprocess the input image\n","def preprocess_image(image_path):\n","    img = load_img(image_path, color_mode='grayscale', target_size=(img_height, img_width))\n","    img_array = img_to_array(img)\n","    img_array = img_array / 255.0  # Normalize pixel values\n","    img_array = tf.expand_dims(img_array, 0)  # Add batch dimension\n","    return img_array\n","\n","def predict_emotion_with_image(image_path):\n","    # Load and display the image\n","    img = load_img(image_path, color_mode='grayscale', target_size=(img_height, img_width))\n","    plt.imshow(img, cmap='gray')\n","    plt.axis('off')\n","    plt.show()\n","    \n","    # Preprocess the image\n","    preprocessed_img = preprocess_image(image_path)\n","    \n","    # Make predictions\n","    predictions = model.predict(preprocessed_img)\n","    predicted_class = tf.argmax(predictions[0]).numpy()\n","    \n","    return predicted_class\n","\n","\n","def class_to_emotion(predicted_emotion):\n","    res_dict = {0: 'angry', \n","                1: 'disgusted',\n","                2: 'fearful',\n","                3: 'happy',\n","                4: 'neutral',\n","                5: 'sad',\n","                6: 'surprised'\n","               }\n","    \n","    return res_dict[predicted_emotion]"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.status.busy":"2024-10-10T03:56:21.336157Z","iopub.status.idle":"2024-10-10T03:56:21.336786Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/emotion-detection-fer/test/surprised/im10.png'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_700\\4223592664.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/kaggle/input/emotion-detection-fer/test/surprised/im10.png\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredicted_emotion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_emotion_with_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Predicted emotion class:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_emotion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_emotion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_700\\3932563604.py\u001b[0m in \u001b[0;36mpredict_emotion_with_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_emotion_with_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Load and display the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'grayscale'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'off'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\Tareq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m       \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m       \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/emotion-detection-fer/test/surprised/im10.png'"]}],"source":["image_path = \"test/surprised/im10.png\"\n","predicted_emotion = predict_emotion_with_image(image_path)\n","print(\"Predicted emotion class:\", class_to_emotion(predicted_emotion))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1028436,"sourceId":1732825,"sourceType":"datasetVersion"}],"dockerImageVersionId":30096,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
